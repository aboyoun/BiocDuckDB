---
title: Using DuckDB-backed Bioconductor Objects
author:
- name: Patrick Aboyoun
  email: aboyounp@gene.com
- name: Aaron Lun
  email: infinite.monkeys.with.keyboards@gmail.com
package: BiocDuckDB
date: "Revised: December 4, 2024"
output:
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{User guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, echo=FALSE}
library(BiocStyle)
self <- Biocpkg("BiocDuckDB");
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
```

# Overview

The BiocDuckDB package provides a set of classes that use DuckDB as a backend
for Bioconductor by extending classes defined in `r Biocpkg("S4Arrays")`,
`r Biocpkg("DelayedArray")`, `r Biocpkg("GenomicRanges")`, and
`r Biocpkg("S4Vectors")`. This package is designed to enhance the functionality
of `r Biocpkg("SummarizedExperiment")` by enabling the use of `DuckDBMatrix`
assays with `DuckDBGranges`/`DuckDBGRangesList` row ranges, and `DuckDBDataFrame`
row and column data.

The primary user-facing classes (`DuckDBMatrix`, `DuckDBDataFrame`,
`DuckDBGRanges`) are defined by a connection to a DuckDB table. Each class
specifies a set of column names for the table that serve as a key to define
dimension names, and a set of column names that define the data populating the
object.

```{r}
library(BiocDuckDB)

args(DuckDBMatrix)
args(DuckDBDataFrame)
args(DuckDBGRanges)
```

`conn`
: Either a character vector containing the paths to parquet, csv, or gzipped
csv data files; a string that defines a duckdb `read_*` data source; or a
`tbl_duckdb_connection` object.

`keycols`
: Either a character vector of column names from `conn` that will define the
dimension names of the object, or a named list of character vectors where the
names of the list define the dimension names and the character vectors define
distinct values for each dimension.

`datacols`
: Either a character vector of column names from `conn` or a named `expression`
that will be evaluated in the context of `conn` that defines the data.

# Illustration using common example datasets

We can familiarize ourselves with the `BiocDuckDB` package by wrapping commonly
used `data.frame`, `array`, and `matrix` objects in `DuckDBDataFrame`,
`DuckDBArray`, and `DuckDBMatrix` objects, respectively. This illustration will
use the following datasets:

1. `mtcars`: `data.frame` with row names
2. `infert`: `data.frame` without row names
3. `Titanic`: `array` with 4 dimensions
4. `state.x77`: `matrix`

## `DuckDBDataFrame` with row names

The `DuckDBDataFrame` class extends `r Biocpkg("S4Vectors")` `DataFrame` to
provide access to DuckDB tables. They may or may not have a key column that
defines the row names of the object. When a key column is supplied, the
resulting `DuckDBDataFrame` object will have row names that can be used for row
subscripting.

```{r}
# Write a csv file to contain the mtcars data
mtcars_tf <- tempfile(fileext = ".csv")
write.csv(cbind(model = rownames(mtcars), mtcars), mtcars_tf, row.names = FALSE)

# Create a DuckDBDataFrame that uses a key column as row names
mtcars_ddb <- DuckDBDataFrame(mtcars_tf, datacols = colnames(mtcars), keycol = "model")

# Since the resulting object is not guaranteed to have the samme row ordering,
# reorder the rows to conform to the original mtcars data
mtcars_ddb <- mtcars_ddb[rownames(mtcars), ]

# Assign metadata
metadata(mtcars_ddb) <-
    list(title = "Motor Trend Car Road Tests",
         description = paste("The data was extracted from the 1974 Motor Trend",
                             "US magazine, and comprises fuel consumption and",
                             "10 aspects of automobile design and performance",
                             "for 32 automobiles (1973â€“74 models)."))

# Assign column descriptions as mcols
mcols(mtcars_ddb) <-
    DataFrame(description = c("Miles/(US) gallon", "Number of cylinders",
                              "Displacement (cu.in.)", "Gross horsepower",
                              "Rear axle ratio", "Weight (1000 lbs)",
                              "1/4 mile time", "Engine (0 = V-shaped, 1 = straight)",
                              "Transmission (0 = automatic, 1 = manual)",
                              "Number of forward gears", "Number of carburetors"),
              row.names = colnames(mtcars_ddb))
```

We extract individual columns as `DuckDBColumn` objects and opporate on them
as if they were ordinary vectors.

```{r}
# Extract the miles per gallon column
mtcars_ddb$mpg

# Use a linear model to motivate DuckDBColumn operations
coeff <- coef(lm(log(mpg) ~ hp + wt, data = mtcars))
hp <- mtcars_ddb$hp
wt <- mtcars_ddb$wt
pred <- exp(coeff["(Intercept)"] + coeff["hp"] * hp + coeff["wt"] * wt)
pred

# Can assign the result back to the DuckDBDataFrame
copy <- mtcars_ddb
copy$mpg_pred <- pred
copy[, c("mpg", "mpg_pred")]
```

## `DuckDBDataFrame` without row names

In most real world scenarios, the data will not have row names. In those cases,
the `DuckDBDataFrame` object will contain a `row_number`-generated column that
will provide some level of row idenification. 

```{r}
# Write a gzipped csv file to contain the infert data
infert_tf <- tempfile(fileext = ".csv.gz")
write.csv(infert, gzfile(infert_tf), row.names = FALSE)

# Create a DuckDBDataFrame that doesn't use a key column as row names
infert_ddb <- DuckDBDataFrame(infert_tf)

# Unlike subscripting by row name, subscripting by row number does not
# preserve row order
infert_ddb[c(8,6,7,5,3,9), ]

# Assign metadata
metadata(infert_ddb) <-
    list(title = "Infertility after Spontaneous and Induced Abortion",
         description = paste("This is a matched case-control study."))

# Assign column descriptions as mcols
mcols(infert_ddb) <-
    DataFrame(description = c("Education level",
                              "Age in years of case",
                              "Parity count",
                              "Number of prior induced abortions",
                              "Case status",
                              "Number of prior spontaneous abortions",
                              "Matched set number",
                              "Stratum number"),
              row.names = colnames(infert_ddb))
```

## `DuckDBArray` with more that two dimensions

The `DuckDBArray` class extends `r Biocpkg("DelayedArray")` `DelayedArray` to
provide access to DuckDB tables. The `DuckDBArray` object uses key columns to
define the dimensions of the array. If the values in the key columns are known,
they can be supplied as named lists to the `keycols` argument to speed up
object creation by avoiding a table scan.

In this example we will write the 4-dimensional `Titanic` dataset to a parquet
file using `BiocDuckDB::writeCoordArray`.

```{r}
# Write a parquet file to contain the titanic data
titanic_tf <- tempfile()
writeCoordArray(Titanic, titanic_tf, datacol = "fate")
list.files(titanic_tf, full.names = TRUE, recursive = TRUE)

# Create a DuckDBArray
titanic_ddb <- DuckDBArray(titanic_tf, datacol = "fate", keycols = c("Class", "Sex", "Age", "Survived"))

# Create a DuckDBArray using pre-defined dimension values
titanic_ddb <- DuckDBArray(titanic_tf, datacol = "fate", keycols = dimnames(Titanic))

# Susbcript by dimension name to learn the fate of the crew
titanic_ddb["Crew", , "Adult", ]
```

## `DuckDBMatrix`

The `DuckDBMatrix` class extends `r Biocpkg("DelayedArray")` `DelayedMatrix` to
provide access to DuckDB tables. The `DuckDBMatrix` object uses columns in the
underlying table to define the row and column names. If the row and column
names are known, they can be supplied as named lists to the `row` and `col` or
`keycols` arguments to speed up object creation by avoiding a table scan.

In this example we can use the `BiocDuckDB::listParquetDirs` helper function to
understand the parquet directory layout.

```{r}
# Write a set of parquet files to contain state data
state_tables <- list(dim1 = data.frame(region = state.region,
                                       division = state.division,
                                       row.names = state.name),
                     dim2 = NULL)
state_tf <- tempfile()
writeCoordArray(state.x77, state_tf, dimtbls = state_tables)
listParquetDirs(state_tf)

# Create a DuckDBMatrix and reorder the rows and columns to conform to state.x77
state_ddb <- DuckDBMatrix(state_tf, row = "dim1", col = "dim2", datacol = "value",
                          dimtbls = state_tables)
state_ddb <- state_ddb[rownames(state.x77), colnames(state.x77)]

# Create a DuckDBMatrix using pre-defined row and column values
keycols <- setNames(dimnames(state.x77), c("dim1", "dim2"))
state_ddb <- DuckDBMatrix(state_tf, datacol = "value", keycols = keycols,
                          dimtbls = state_tables)
```

We can extract sub-matrices from the underlying parquet data by either reading
from a subdirectory or using a `DuckDBDataFrame` to filter the data prior to
constructing the matrix.

```{r}
# Use the data files for just the Western states
western_state_ddb <- DuckDBMatrix(file.path(state_tf, "region=West"), row = "dim1", col = "dim2", datacol = "value")
western_state_ddb <- western_state_ddb[sort(rownames(western_state_ddb)), colnames(state.x77)]

# Use a DuckDBDataFrame to filter the Southern states
full_df <- DuckDBDataFrame(state_tf)
southern_df <- full_df[full_df$region == "South", ]
southern_state_ddb <- DuckDBMatrix(southern_df, row = "dim1", col = "dim2", datacol = "value")
southern_state_ddb <- southern_state_ddb[sort(rownames(southern_state_ddb)), colnames(state.x77)]
```

## `DuckDBArray` using a random sparse array

The `DuckDBArray` and `DuckDBMatrix` classes support sparse arrays and matrices
in a seemless manner. The following toy example creates a 26 x 26 x 12 sparse
array, but this can be scaled to out to accommodate larger data.

```{r}
set.seed(123)
ind <- arrayInd(sort(sample.int(26 * 26 * 12, 1000)), c(26, 26, 12))
sparse_df <- data.frame(dim1 = letters[ind[, 1]],
                        dim2 = LETTERS[ind[, 2]],
                        dim3 = month.abb[ind[, 3]],
                        value = sample(10L, 1000, replace = TRUE, prob = 2^(10:1)))
sparse_tf <- tempfile(fileext = ".parquet")
arrow::write_parquet(sparse_df, sparse_tf)

# Create a DuckDBMatrix
sparse_ddb <- DuckDBArray(sparse_tf, datacol = "value", keycols = list("dim1" = letters, "dim2" = LETTERS, "dim3" = month.abb))
sparse_ddb
```

# Creating DuckDB-backed `SummarizedExperiment` objects

## Basic `SummarizedExperiment` with DuckDB-backed components

We can now use the BiocDuckDB classes to manage and analyze large datasets
efficiently within the Bioconductor ecosystem. For example, we could represent
the `airway` example dataset from `r Biocpkg("SummarizedExperiment")` as a set
of parquet tables to illustrate how to assemble a DuckDB-backed
`SummarizedExperiment` object.

By writing the row ranges, row data, column data, and assay data to DuckDB
tables, we can create `DuckDBGRanges`, `DuckDBDataFrame`, and `DuckDBMatrix`
objects that are backed by DuckDB tables. This setup allows for scalable data
storage and computation, facilitating efficient manipulation and analysis of
large datasets. The final `SummarizedExperiment` object, `airway_ddb`, is
constructed using these DuckDB-backed components, demonstrating the seamless
integration of DuckDB with Bioconductor's data structures.

```{r}
library(SummarizedExperiment)
data(airway, package = "airway")

airway_rowranges_td <- tempfile(pattern = "rowranges_")
arrow::write_dataset(as.data.frame(rowRanges(airway)), airway_rowranges_td, format = "parquet")

airway_rowdata_td <- tempfile(pattern = "rowdata_")
arrow::write_dataset(as.data.frame(rowData(airway)), airway_rowdata_td, format = "parquet")

airway_coldata_td <- tempfile(pattern = "coldata_")
arrow::write_dataset(as.data.frame(colData(airway)), airway_coldata_td, format = "parquet")

airway_counts_td <- tempfile(pattern = "counts_")
writeCoordArray(assays(airway)[["counts"]], airway_counts_td, keycols = c("gene_id", "run"))

rranges <- DuckDBGRanges(airway_rowranges_td, seqnames = "seqnames", start = "start", end = "end",
                         strand = "strand", mcols = c("exon_id", "exon_name", "group_name"),
                         seqinfo = seqinfo(airway))

rdata <- DuckDBDataFrame(airway_rowdata_td, keycol = "gene_id")
rdata <- rdata[sort(rownames(rdata)), ]

rranges <- split(rranges, rranges$group_name)
mcols(rranges) <- rdata

cdata <- DuckDBDataFrame(airway_coldata_td, keycol = "Run")
cdata <- cdata[sort(rownames(cdata)), ]

counts <- DuckDBMatrix(airway_counts_td, row = "gene_id", col = "run", datacol = "value")
counts <- counts[rownames(rdata), sort(rownames(cdata))]

airway_ddb <- SummarizedExperiment(assays = SimpleList(counts = counts),
                                   rowRanges = rranges, colData = cdata,
                                   metadata = metadata(airway))
```

## `SingleCellExperiment` object with DuckDB-backed components

We can also use the BiocDuckDB classes to manage and analyze single-cell RNA-seq
datasets efficiently within the Bioconductor ecosystem. For example, we could
represent the `ZilionisLungData` dataset from the `r Biocpkg("scRNAseq")`
package as a set of parquet tables to illustrate how to assemble a
DuckDB-backed `SingleCellExperiment` object.

By writing the column data, counts matrix, and reduced dimensions to DuckDB
tables, we can create `DuckDBDataFrame` and `DuckDBMatrix` objects that are
backed by DuckDB tables. This setup allows for scalable data storage and
computation, facilitating efficient manipulation and analysis of large
single-cell RNA-seq datasets. The final `SingleCellExperiment` object,
`zilionis`, is constructed using these DuckDB-backed components, demonstrating
the seamless integration of DuckDB with Bioconductor's data structures.

```{r, eval}
library(scRNAseq)
zilionis <- ZilionisLungData()

zilionis_path <- file.path(tempdir(), "zilionis")

zilionis_cdata_path <- file.path(zilionis_path, "coldata")
zilionis_counts_path <- file.path(zilionis_path, "assays", "assay=counts")
zilionis_rdims_path <- file.path(zilionis_path, "reduced_dims")

# Column metadata
arrow::write_dataset(as.data.frame(colData(zilionis)), zilionis_cdata_path, format = "parquet")

# Counts matrix
dimtbls <- list(NULL, colData(zilionis)[, "Library", drop = FALSE])
writeCoordArray(counts(zilionis), zilionis_counts_path, keycols = c("Genename", "Barcode"),
                datacol = "Value", dimtbls = dimtbls, partitioning = NULL)

# Reduced dimensions
dimtbls <- list(colData(zilionis)[, "Library", drop = FALSE], NULL)
for (rd in reducedDimNames(zilionis)) {
    mat <- reducedDim(zilionis, rd)
    mat[is.na(mat)] <- 0 # Convert NAs to 0 to create a sparse matrix
    path <- file.path(zilionis_rdims_path, sprintf("type=%s", rd))
    writeCoordArray(mat, path, keycols = c("Barcode", "Dim"), datacol = "Value",
                    dimtbls = dimtbls, partitioning = NULL)
}
```

This code block demonstrates how to construct a `SingleCellExperiment` object
from the DuckDB-backed components. The `zilionis_ddb` object is created using
`DuckDBDataFrame` and `DuckDBMatrix` objects that are backed by DuckDB tables.
Since the original `SingleCellExperiment` object, `zilionis`, does not have
unique sample identifiers, we concatenate the `Library` and `Barcode` metadata
columns to create one.

```{r}
library(dplyr)
tbl <- mutate(tblconn(DuckDBTable(zilionis_cdata_path)),
              ID = concat_ws("_", Library, Barcode),
              row_number = NULL)

zilionis_coldata <- DuckDBDataFrame(tbl, datacols = setdiff(colnames(tbl), "ID"), keycol = "ID")
zilionis_coldata <- zilionis_coldata[paste(colData(zilionis)[, "Library"],
                                           colData(zilionis)[, "Barcode"], sep = "_"), ]

tbl <- mutate(tblconn(DuckDBTable(zilionis_counts_path)),
              ID = concat_ws("_", Library, Barcode),
              row_number = NULL)
zilionis_counts <- DuckDBMatrix(tbl, row = "Genename",
                                col = list("ID" = rownames(zilionis_coldata)),
                                datacol = "Value")
zilionis_counts <- zilionis_counts[rownames(zilionis), ]

zilionis_rdims <-
    sapply(reducedDimNames(zilionis), function(rd) {
        tbl <- mutate(tblconn(DuckDBTable(file.path(zilionis_rdims_path, sprintf("type=%s", rd)))),
                      ID = concat_ws("_", Library, Barcode),
                      row_number = NULL)
        mat <- DuckDBMatrix(tbl, row = list("ID" = rownames(zilionis_coldata)),
                            col = "Dim", datacol = "Value")
        mat[, c("x", "y")]
    }, simplify = FALSE)

zilionis_ddb <- SingleCellExperiment(assays = list(counts = zilionis_counts),
                                     colData = zilionis_coldata,
                                     reducedDims = zilionis_rdims)
```

# Retrieving the DuckDB table connection

At any point, users can retrieve the query to the underlying DuckDB table via the
`dbconn` method. This can be used with methods in the `r CRANpkg("duckdb")` package
to push more complex operations to the DuckDB for greater efficiency.

```{r}
dbconn(mtcars_ddb)
```

# Session information {-}

```{r}
sessionInfo()
```
